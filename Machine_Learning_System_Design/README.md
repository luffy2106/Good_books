### The summary of the book Machine Learning System Design - Huyen Chip

#### 3. Data engineering fundamentals
##### 3.1 Data sources

- ML systems use data from multiple sources with varying characteristics and purposes, requiring different processing methods.
- Understanding data sources enhances efficient data usage in ML systems.
- Key data sources include:
  - **User Input Data:** 
    - Includes text, images, videos, and uploaded files.
    - Prone to malformation and requires thorough validation and fast processing.
  - **System-Generated Data:**
    - Includes logs and system outputs like model predictions.
    - Provides visibility for debugging and improvement.
    - Typically less malformatted and can be processed periodically.
    - Can grow quickly, creating challenges in identifying relevant information and storage.
    - Logging extensively is common for thorough debugging, with services like Logstash, Datadog, and Logz.io aiding in log analysis.
  - **Behavioral Data:**
    - Records user actions (clicks, scrolling, etc.) and is subject to privacy regulations.
  - **Internal Databases:**
    - Generated by services and applications within a company.
    - Used by ML models for tasks like inventory checks and search query processing.
  - **Third-Party Data:**
    - Includes data collected by other companies or public data, often requiring payment.
    - Used for insights like social media activities, purchase history, and demographic analysis.
    - Privacy regulations (like Apple’s IDFA opt-in policy) have reduced availability, pushing companies to rely more on first-party data.
    - Advertisers are developing workarounds, such as the China Advertising Association’s CAID system for device fingerprinting.

##### 3.2 Data formats

- **Data Storage Considerations:**
  - Storing data from multiple sources can be complex and costly.
  - Consider the future use of data to choose an appropriate storage format.
  - Key questions include:
    - How to store multimodal data (images, text).
    - Cost-effective yet fast data storage options.
    - Storing complex models for different hardware compatibility.

- **Data Serialization:**
  - Converts data structures into a storable/transmittable format.
  - Important characteristics of serialization formats: human readability, access patterns, text vs. binary.

- **Common Data Formats:**
  - **JSON:** Text-based, human-readable, widely used, but schema changes are difficult.
  - **CSV:** Text-based, row-major format, better for accessing rows.
  - **Parquet:** Binary, column-major format, better for accessing columns.

- **Row-Major vs. Column-Major Formats:**
  - Row-major (e.g., CSV) is faster for row access and data writes.
  - Column-major (e.g., Parquet) is faster for column-based reads and large datasets.

- **Text vs. Binary Formats:**
  - Text files (e.g., CSV, JSON) are human-readable but larger.
  - Binary files (e.g., Parquet) are more compact and efficient.

- **Relational Data Model:**
  - Organizes data into tables (relations) with rows (tuples).
  - Normalization reduces redundancy but spreads data across tables.
  - SQL is the primary query language, declarative in nature, with query optimization being crucial.

- **Declarative ML Systems:**
  - Inspired by declarative data systems, aim to simplify ML tasks.
  - Examples: Ludwig (Uber), H2O AutoML, which automate model selection and training.

- **NoSQL Data Models:**
  - **Document Model:**
    - Stores data in self-contained documents (e.g., JSON, BSON).
    - Flexible, schemaless but shifts schema assumption to the reading application.
  - **Graph Model:**
    - Focuses on relationships between data items, using nodes and edges.
    - Efficient for relationship-based queries.

- **Structured vs. Unstructured Data:**
  - **Structured Data:**
    - Follows a predefined schema, easier to analyze.
    - Stored in data warehouses(ex : Google BigQuerry, SnowFlake)
  - **Unstructured Data:**
    - Does not follow a schema, more flexible.
    - Stored in data lakes(ex: Amazon S3 (Simple Storage Service) with AWS Lake Formation, Azure Data Lake Storage (ADLS))
  - Data warehouses are for processed data; data lakes store raw data.

##### 3.3 Data Storage Engines and Processing

- **Data Storage Engines and Processing:**
  - Data formats and models define how data is stored/retrieved.
  - Storage engines (databases) implement these processes.
  - Understanding different databases is crucial for application-specific needs.

- **Transactional vs. Analytical Processing:**
  - **Transactional Processing (OLTP):**
    - Handles individual actions (e.g., tweeting, ordering a ride).
    - Requires low latency and high availability.
    - Often uses ACID principles (Atomicity, Consistency, Isolation, Durability).
  - **Analytical Processing (OLAP):**
    - Handles data aggregation and analysis.
    - Efficient for queries involving data from multiple rows/columns.
  - Modern databases often blend OLTP and OLAP capabilities (e.g., CockroachDB, Apache Iceberg, DuckDB).

- **ETL (Extract, Transform, Load) Process:**
  - Essential for ML system data handling.
  - **Extract:**
    - Retrieve data from various sources, validate, and reject corrupted data.
  - **Transform:**
    - Clean, standardize, and process data (joining, deduplicating, aggregating).
  - **Load:**
    - Load transformed data into target destinations (databases, data warehouses).
  - ETL is foundational for data processing in organizations.

- **Evolution of Data Storage:**
  - Growth of data sources and changing schemas led to storing raw data in data lakes.
  - ELT (Extract, Load, Transform) stores data first, processes later, allowing fast data arrival.
  - Challenges with raw data search inefficiency and evolving infrastructures have led to hybrid solutions combining data lakes and warehouses (e.g., Databricks, Snowflake's data lakehouse).

- **Key Concepts:**
  - **Transactional Databases:**
    - Typically row-major, efficient for individual transactions but not for aggregating data.
  - **Analytical Databases:**
    - Efficient for aggregating and analyzing large datasets.
  - **Decoupling Storage from Processing:**
    - Allows storing data in one place with different processing layers for various queries.

- **Terminology:**
  - **ACID vs. BASE:**
    - ACID: Strong consistency and reliability.
    - BASE: More flexibility with eventual consistency.
  - **Online, Nearline, Offline Processing:**
    - Online: Immediate data availability.
    - Nearline: Quick data availability without human intervention.
    - Offline: Requires human intervention for data access.

##### 3.4 Dataflow Modes

- **Dataflow Modes:**
  - **Data Passing Through Databases:**
    - Data is written by one process and read by another from a shared database.
    - Limitations: both processes must access the same database and database read/write speeds may not meet latency requirements.
  
  - **Data Passing Through Services:**
    - Data is exchanged through network requests (e.g., REST, RPC).
    - Used in service-oriented and microservice architectures.
    - Allows independent development and maintenance of different application components.
    - Examples: Ride-sharing app services (driver management, ride management, price optimization).
  
  - **Data Passing Through Real-Time Transport:**
    - Data is broadcast to an intermediary broker rather than directly between services.
    - Reduces complexity and dependency issues in interservice communication.
    - More efficient for data-heavy systems with strict latency requirements.
    - Examples of real-time transport: Apache Kafka, Amazon Kinesis (pubsub), Apache RocketMQ, RabbitMQ (message queue).

- **Request-Driven Architecture:**
Request-driven architecture, often seen in RESTful APIs and microservices architectures, focuses on systems where clients send requests to services, and services respond with the requested data or action. This type of architecture is synchronous, meaning the client waits for the server to process the request and return a response. Here's an example to illustrate request-driven architecture:
    - User Action: A user opens the ShopSmart app and searches for "smartphones."
    - Client Request: The frontend app sends a GET request to the API Gateway endpoint /products?search=smartphones.
    - API Gateway: Forwards the request to the Product Service.
    - Product Service: Queries the database for products matching "smartphones" and returns the list to the API Gateway.
    - Response: The API Gateway sends the product list back to the frontend app, displaying it to the user.
Key features:
    - Services communicate via direct requests.
    - Suitable for logic-heavy systems.
    - Potential issues: complexity in communication, dependency on service availability, and potential bottlenecks.
- **Event-Driven Architecture:**
  - Uses brokers to manage data passing (in-memory storage for real-time transport).
  - Suitable for data-heavy systems.
  - Data broadcast to a real-time transport is called an event; the system is also called an event bus.
  - Pubsub model: Services publish to topics and any subscribing service can read the events.
  - Message queue model: Events (messages) have intended consumers, and the queue ensures delivery to the right consumers.
  
- **Examples and Popular Implementations:**
  - **Pubsub Solutions:**
  In a pub-sub model, message senders (publishers) send messages to a topic, and multiple receivers (subscribers) receive those messages by this topic. This model decouples the producers and consumers of messages, allowing for a more flexible and scalable architecture.
  Use cases example:
  * Publisher: User actions such as posting a status update or liking a photo.
  * Subscribers: Followers of the user who need to be notified of the update
  * Flow:
    - When a user posts a status update, the action is published to a topic.
    - All followers of the user are subscribed to this topic and receive notifications in real-time.  
  Tool example:
    - Apache Kafka
    - Amazon Kinesis
  - **Message Queue Solutions:**
  In a message queue model, messages are stored in a queue and processed by consumers in a first-in, first-out (FIFO) order. This model is designed for point-to-point communication, ensuring that each message is processed by exactly one consumer.
  Use Case Example: Order Processing System in an E-Commerce App:
  * Producer: The order placement service that generates new orders.
  * Consumer: The order fulfillment service that processes and ships orders.
  * Flow:
    - When a customer places an order, the order details are sent to a message queue.
    - The order fulfillment service reads the order from the queue and processes it.
    - The message is removed from the queue once it is successfully processed.
  Another example could be taken is E-cataloging process, when we ask the users validate the result of the model
  Tool example:
    - Apache RocketMQ
    - RabbitMQ
  - **The main difference PubSub and Mesage Queue**
    - Pub-Sub Solutions: Ideal for scenarios where messages need to be broadcast to multiple subscribers in real-time, such as notifications or real-time updates.
    - Message Queue Solutions: Best suited for scenarios requiring reliable, point-to-point communication with message persistence and load balancing, such as task processing or order fulfillment.

- **Advantages of Real-Time Transport:**
  - Simplifies communication between services.
  - Decouples service dependencies.
  - Enhances scalability and resilience.
  - Ensures timely data availability for applications with strict latency requirements.

##### 3.5 Batch Processing Versus Stream Processing 
- **Historical Data vs. Streaming Data:**
  - **Historical Data:** Stored in data storage engines (databases, data lakes, data warehouses) and processed in batch jobs.
  - **Streaming Data:** Continuously flows through real-time transports like Apache Kafka and Amazon Kinesis, processed in real-time.

- **Batch Processing:**
  - Processes historical data periodically (e.g., daily).
  - Efficient for static features (e.g., driver ratings).
  - Uses distributed systems like MapReduce and Spark.
  - Suitable for features that change less frequently.
  - Known as static features.

- **Stream Processing:**
  - Processes streaming data in real-time or in short intervals (e.g., every five minutes).
  - Ideal for dynamic features (e.g., current number of available drivers).
  - Low latency as it processes data as soon as it is generated.
  - Uses scalable technologies like Apache Flink, KSQL, and Spark Streaming.
  - Suitable for features that change quickly.
  - Known as dynamic features.

- **Advantages of Stream Processing:**
  - Provides low latency by processing data immediately.
  - Efficient for stateful computation, preventing redundancy.
  - Highly scalable and capable of parallel computation.
  - Suitable for applications requiring frequent updates, such as fraud detection and credit scoring.

- **Challenges in Stream Processing:**
  - Handling unbounded data with variable rates and speeds.
  - More complex than batch processing due to continuous data flow.

- **Combining Batch and Stream Processing:**
  - Many problems require both static (batch) and dynamic (streaming) features.
  - Infrastructure needed to process and join both types of data for ML models.
  - Apache Flink and KSQL provide SQL abstraction, useful for complex stream feature extraction.

- **Stream Computation Engines:**
  - **Apache Kafka:** Built-in stream computation capabilities, limited with various data sources.
  - **Apache Flink and KSQL:** Recognized for efficient stream processing with SQL abstraction.
  - **Spark Streaming:** Another option for stream processing.

- **Conclusion:**
  - Batch processing is simpler but less frequent and suited for static data.
  - Stream processing is complex, more frequent, and suited for dynamic data.
  - Combining both processing types enhances the capability to generate accurate ML model predictions.

#### 4. Training data

##### 4.1 Sampling

- **Importance of Sampling in ML:**
  - Integral to various stages of ML projects: creating training data, data splitting, and monitoring.
  - Necessary when all real-world data isn’t available or when processing all data is infeasible.
  - Sampling methods improve efficiency and help avoid biases.

- **Types of Sampling:**
  - **Nonprobability Sampling:**
    - Convenience sampling: Based on data availability.
    - Snowball sampling: Future samples selected based on existing samples.
    - Judgment sampling: Experts select samples.
    - Quota sampling: Samples based on quotas without randomization.
    - Often biased and not representative of real-world data.
    - Common in initial data gathering for projects.

  - **Probability-Based Sampling:**
    - **Simple Random Sampling:**
      - Equal probability for all samples.
      - Easy to implement but may miss rare categories.
    - **Stratified Sampling:**
      - Divides population into groups (strata) and samples from each.
      - Ensures representation of all groups but can be challenging for multilabel tasks.
    - **Weighted Sampling:**
      - Samples assigned weights to influence selection probability.
      - Useful for domain expertise and adjusting for distribution differences.
      - Related to sample weights in ML for influencing model training.
    - **Reservoir Sampling:**
      - Useful for streaming data.
      - Ensures equal probability for each sample even when the total data size is unknown.
      - Algorithm maintains a reservoir of samples with correct probabilities.
    - **Importance Sampling:**
      - Samples from an easier distribution (Q) and weights them to match the target distribution (P).
      - Common in policy-based reinforcement learning to update policies efficiently.

- **Nonprobability Sampling Examples:**
  - Language modeling often uses easily collected data (e.g., Wikipedia, Reddit).
  - Sentiment analysis uses naturally labeled data (e.g., IMDB, Amazon reviews).
  - Self-driving cars initially collected data from limited geographic areas.

- **Simple Random Sampling Example:**
  - Randomly selecting 10% of a population ensures equal chances but may miss rare categories.

- **Stratified Sampling Example:**
  - Sampling 1% from two classes ensures both are represented, useful in unbalanced datasets.

- **Weighted Sampling Example:**
  - Assigning higher weights to more recent data or underrepresented categories to reflect real-world distribution.

- **Reservoir Sampling Example:**
  - Sampling tweets from an incoming stream without knowing the total number of tweets.

- **Importance Sampling Example:**
  - In reinforcement learning, using old policy rewards as a proposal distribution for new policy updates.

Understanding and choosing the appropriate sampling method is crucial for creating reliable and efficient ML models.


##### 4.2 Labelling
The text discusses the challenges and methodologies associated with obtaining labeled data for machine learning (ML) models. Despite the promise of unsupervised ML, most models today rely on supervised learning, requiring high-quality labeled data for effective training. Key points include:

**Importance of Labeled Data**:
  - Performance of ML models depends on the quality and quantity of labeled data.
  - Data labeling is now a core function of many ML teams.

**Hand Labeling**:
  - Expensive and time-consuming, especially when expertise is required (e.g., radiologists for medical data).
  - Raises privacy concerns, as data must be shared with annotators.
  - Slow process, leading to slower model adaptation to changes.

**Label Multiplicity**:
  - Different annotators often provide conflicting labels for the same data, causing label ambiguity.
  - Clear problem definitions and consistent training for annotators can reduce disagreements.

**Data Lineage**:
  - Tracking the origin and quality of labeled data is crucial to avoid model performance issues when integrating new data.

**Natural Labels**:
  - Some tasks have natural ground truth labels inferred from system interactions (e.g., Google Maps ETA, stock price predictions, recommender systems).
  - Feedback loop length varies; shorter loops allow quicker model adaptation, while longer loops are common in tasks like fraud detection.

**Handling Lack of Labels**:
  - **Weak Supervision**: Uses heuristics and labeling functions to generate noisy but useful labels. Tools like Snorkel aid this process.
  - **Semi-Supervision**: Combines small amounts of labeled data with assumptions to generate more labels.
  - **Transfer Learning**: Uses pretrained models as a starting point for new tasks, reducing the need for large amounts of labeled data.
  - **Active Learning**: Focuses on labeling the most informative data samples to improve model efficiency with fewer labels.

**Challenges and Strategies**:
  - Addressing hand labeling issues involves cost, privacy, and speed considerations.
  - Techniques like weak supervision, semi-supervision, transfer learning, and active learning help mitigate the lack of labeled data.
  - Active learning is particularly promising for real-time data adaptation.

Overall, the text emphasizes the critical role of data labeling in ML, the challenges faced in obtaining labeled data, and various strategies to overcome these challenges to improve model performance.


##### 4.3 Class imbalance
[to be continued ]



#### 5. Feature engineering
[to be continued ]

#### 6. Model Development and Offline Evaluation
[to be continued ]


#### 7. Model Deployment and Prediction Service

##### 7.1 Batch Prediction Versus Online Prediction
The text provides an in-depth discussion on the differences and implications of using batch prediction versus online prediction in machine learning (ML) systems. Key points include:

**Prediction Modes**:
  - **Batch Prediction**: Generates predictions periodically or when triggered, storing them for future use. Known as asynchronous prediction.
  - **Online Prediction**: Generates predictions in real-time as requests arrive, typically via RESTful APIs. Known as synchronous prediction.
  - **Streaming Prediction**: A type of online prediction that uses both batch and streaming features.

**Batch vs. Online Prediction**:
  - **Batch Prediction**:
    - Uses historical (batch) data.
    - Suitable for scenarios where immediate results are not required (e.g., periodic movie recommendations).
    - Optimized for high throughput but less responsive to real-time changes in user preferences.
    - Can reduce inference latency for complex models by precomputing predictions.
  - **Online Prediction**:
    - Uses real-time (streaming) data and batch data.
    - Necessary for applications requiring immediate results (e.g., fraud detection, autonomous vehicles).
    - Optimized for low latency but traditionally considered less efficient in terms of cost and performance.

**Terminology Confusion**:
  - Terms like “online prediction” and “batch prediction” can be confusing. The text suggests using “synchronous prediction” and “asynchronous prediction” but acknowledges that even these terms are not perfect.

**Use Cases**:
  - **Batch Prediction**: Used for generating large volumes of predictions, such as recommendations for all users periodically.
  - **Online Prediction**: Used for generating predictions on-demand, essential for applications like real-time language translation and high-frequency trading.

**Hybrid Solutions**:
  - Some systems precompute predictions for popular queries (batch prediction) and generate predictions online for less popular queries, combining the benefits of both approaches.

**Transition from Batch to Online Prediction**:
  - Online prediction is often the first approach used during prototyping due to its immediate feedback nature.
  - Batch prediction can be a workaround for scenarios where online prediction is too slow or costly.
  - As hardware and techniques improve, the trend is moving towards more online predictions.

**Challenges and Infrastructure**:
  - Building a unified pipeline for both batch and streaming data is complex but essential for reducing bugs and maintaining consistency.
  - Companies are investing in stream processing frameworks and feature stores to achieve this unification.

**Key Differences**:
  - Batch prediction processes accumulated data periodically, useful for recommendations.
  - Online prediction processes data as soon as it arrives, critical for real-time applications.

##### 7.2 Model compression

The text explores various techniques for reducing the inference latency of machine learning (ML) models, emphasizing model compression and optimization. Here are the key points:

**Inference Optimization Approaches**:
  - **Make inference faster**: Improve the efficiency of the inference process.
  - **Make the model smaller**: Reduce the model size through compression techniques.
  - **Improve hardware performance**: Use faster hardware for deploying the model.

**Model Compression**:
  - **Low-Rank Factorization**: Replaces high-dimensional tensors with lower-dimensional ones to reduce parameters and increase speed. Examples include compact convolutional filters and MobileNets.
  - **Knowledge Distillation**: A smaller model (student) is trained to mimic a larger model (teacher), retaining much of the teacher's capabilities but with reduced size and faster inference. Example: DistilBERT.
  - **Pruning**: Removes less important parameters from the model, making it sparser and more efficient without significantly compromising accuracy. This can involve setting certain parameters to zero or removing entire nodes.
  - **Quantization**: Reduces the number of bits used to represent model parameters, thus decreasing memory usage and improving computation speed. Commonly involves using lower precision (e.g., 16-bit or 8-bit integers) instead of the default 32-bit floats.

**Detailed Techniques**:
  - **Low-Rank Factorization**: Used mainly for convolutional neural networks, it involves strategies like replacing 3x3 convolutions with 1x1 convolutions.
  - **Knowledge Distillation**: Useful for deploying smaller models, it can work across different architectures but requires an existing larger model as a teacher.
  - **Pruning**: Can significantly reduce the nonzero parameters of a neural network, making it more efficient.
  - **Quantization**: General and widely used, it reduces the precision of model parameters, resulting in faster computations and reduced memory footprint. It can be applied during or after training.

**Quantization Details**:
  - **Half Precision (16-bit)**: Reduces memory usage by half compared to 32-bit.
  - **Fixed Point (8-bit)**: Further reduces memory usage and improves speed. Binary weight neural networks represent an extreme case with 1-bit weights.
  - **Trade-offs**: Lower precision can lead to rounding errors and under/overflow issues, which need careful handling.

**Case Study - Roblox**:
  - Roblox scaled BERT to handle over 1 billion daily requests on CPUs by implementing various compression techniques.
  - They started with a large BERT model, then used DistilBERT and dynamic shape input, and finally quantized the model.
  - Quantization provided the most significant performance boost, reducing latency sevenfold and increasing throughput eightfold.

##### 7.3 ML on the Cloud and on the Edge

The text discusses the considerations and trade-offs between deploying machine learning (ML) models on the cloud versus on edge devices. 

### Key Points:

**Cloud vs. Edge Computation**:
  - **Cloud Computation**: Involves performing a large portion of computations on public or private cloud servers.
  - **Edge Computation**: Involves performing computations on consumer devices (e.g., phones, laptops, cars, smartwatches).

**Cloud Deployment**:
  - **Ease of Use**: Managed cloud services (AWS, GCP) make it straightforward to deploy ML models.
  - **Cost Concerns**: Cloud computations can be expensive. Large companies spend millions annually, and small/medium companies also incur significant costs. Mishandling cloud services can financially strain startups.

**Edge Deployment**:
  - **Cost Efficiency**: Reduces the amount of computation required on the cloud, thereby lowering costs.
  - **Operational Benefits**:
    - Works without internet or with unreliable connections, making it viable in remote areas or places with strict no-internet policies.
    - Reduces network latency as computations occur locally on the device, improving real-time performance.
  - **Privacy and Security**: Keeps user data on local devices, mitigating risks of data breaches and aiding compliance with data regulations (e.g., GDPR). However, physical device theft remains a risk.
  - **Device Requirements**: Edge devices need sufficient computational power, memory, and battery life to handle ML models.

**Trends and Developments**:
  - **Rise of Edge Devices**: Companies are developing specialized hardware for ML use cases (e.g., Google, Apple, Tesla), and the number of active edge devices is projected to exceed 30 billion by 2025.
  - **Optimization Challenges**: Running ML models efficiently on diverse hardware requires significant effort. Intermediate representations (IRs) and compilers help bridge frameworks and hardware platforms, simplifying this process.

**Model Optimization**:
  - **Compilers and IRs**: Translate high-level model code into machine code specific to hardware backends. This involves optimizing computation graphs for efficiency.
  - **Techniques**: Include vectorization, parallelization, loop tiling, and operator fusion to enhance performance.
  - **ML for Optimization**: Tools like autoTVM use ML to optimize models by predicting the best execution paths, though this process can be time-consuming.
In the end, after optimizing model for specific type of GPU, you can have a model which is faster for the inference.

**ML in Browsers**:
  - **WebAssembly (WASM)**: Allows running ML models in browsers, making them hardware-agnostic. While WASM is faster than JavaScript, it is still slower than native applications.


#### 8. Data distribution shifts and monitoring
A company deployed an ML model to predict grocery demand, initially achieving good results. Over time, the model's accuracy declined, leading to inventory mismanagement.
The company faced expensive options to update the model or build an in-house team. Lession leanned:
- Deploying an ML model requires ongoing monitoring and updates to maintain performance.
- Changes in data patterns over time can cause models to fail, highlighting the need for regular updates.

##### 8.1. Causes of ML System Failures

- Failure: Occurs when one or more system expectations are violated.
- Operational Expectations: Similar to traditional software, focusing on metrics like latency and throughput.
- ML Performance Expectations: Unique to ML systems, focusing on the accuracy and quality of predictions.

**Types of Failures**
- Software System Failures: 
  - Dependency Failure: Breaks due to third-party dependencies.
  - Deployment Failure: Errors during deployment, such as incorrect binaries or permissions issues.
  - Hardware Failures: Failures due to malfunctioning hardware like CPUs or GPUs.
  - Downtime or Crashing: System failures due to server downtimes.
  - Predominantly non-ML related but essential for ML engineers to address using traditional software engineering skills.
  - Example: Google’s ML pipeline failures often stemmed from distributed system or data pipeline issues.

- ML-Specific Failures:
  - Data Collection and Processing Problems: Issues arising from incorrect or inadequate data.
  - Poor Hyperparameters: Suboptimal settings leading to poor model performance.
  - Training-Inference Pipeline Discrepancies: Mismatches between training and inference environments.
  - Data Distribution Shifts: Changes in data over time that the model was not trained to handle.
  - Edge Cases: Rare, extreme cases that the model cannot handle well.
  - Degenerate Feedback Loops: Situations where model outputs influence future inputs, leading to biased or suboptimal performance.

#### Detailed ML-Specific Failures
- Production Data Differing from Training Data:
  - Generalization Issue: Training data might not represent the real-world data accurately.
  - Real-World Data Variability: Real-world data can shift over time, making initial training data insufficient.
  - Examples: Shifts due to events like COVID-19, seasonal changes, or internal errors like bugs in the data pipeline.

- Edge Cases:
  - High-Risk Scenarios: ML models failing in critical scenarios (e.g., self-driving cars, medical diagnosis).
  - Outliers vs. Edge Cases: Outliers are data points significantly different from others, while edge cases are where the model performs poorly on a rare case, but it have big effect.

- Degenerate Feedback Loops:
  - Feedback Influence: When model predictions affect user interactions, which in turn affect future predictions.
  - Common in Recommender Systems: Items getting more exposure due to initial high ranking, leading to popularity bias.
  - Examples in recommeding ads: some ads keep showing ups in the website just because the AI thinks it's the most interesting for the users, and the users keep clicking it just because they don't see anything else show, which make the AI even more belive that its algorithm is correct.

- Detecting and Correcting Degenerate Feedback Loops
  - Detection: Measure diversity and popularity of system outputs; observe if predictions become homogeneous over time.
  - Correction Methods:
      - Randomization: Introducing randomness in predictions to reduce homogeneity.
      - Positional Features: Using features that account for the position of an item in a recommendation list to mitigate bias.
      - Two-Model Approach: Separate models for predicting visibility and interaction likelihood. First model to predict the probability that a user will see and consider an ad based on its position. The second model that predict the probability that a user will click on an ad given that they have seen and considered it.In the end, we rank the ads based on the combined probabilities from both models.

##### 8.2. Data distribution shifts

The text provides an in-depth analysis of data distribution shifts in supervised machine learning (ML) systems, which occur when the data a model is trained on differs from the data it encounters in production, causing a decline in predictive accuracy over time. It emphasizes the importance of understanding and addressing these shifts to maintain model performance.

### Key Points:

**Definition and Types of Data Distribution Shifts**:
  - **Covariate Shift**: Change in the distribution of the input data (\(P(X)\)) while the relationship between inputs and outputs (\(P(Y|X)\)) remains the same.
  - **Label Shift**: Change in the distribution of the output data (\(P(Y)\)) while the relationship between outputs and inputs (\(P(X|Y)\)) remains the same.
  - **Concept Drift**: Change in the relationship between inputs and outputs (\(P(Y|X)\)) while the input distribution (\(P(X)\)) remains the same.

**Causes and Examples**:
  - Covariate shifts can arise from biases in data collection, changes in the environment, or artificial data alterations for model training.
  - Label shifts often occur alongside covariate shifts but can happen independently, such as when a preventive measure changes disease incidence rates.
  - Concept drift can result from significant events (e.g., COVID-19 affecting housing prices) or cyclic/seasonal variations.

**Detection of Data Shifts**:
  - Monitoring accuracy-related metrics in production is crucial but challenging due to the lack of real-time ground truth labels.
  - Statistical methods and two-sample tests (e.g., Kolmogorov-Smirnov test) can help detect shifts by comparing training and production data distributions.
  - Importance of time scale windows in detecting temporal shifts, with cumulative and sliding statistics aiding in monitoring.

**Addressing Data Shifts**:
  - Regular retraining of models using updated data to adapt to new distributions.
  - Using massive datasets to train robust models that generalize well to new data.
  - Advanced techniques like domain adaptation and transfer learning for model adjustment without extensive retraining.
  - Designing systems to be more resilient to shifts by carefully selecting and managing features and employing separate models for different segments.

**Practical Considerations**:
  - Companies must balance model performance and feature stability to minimize frequent retraining.
  - Addressing human errors and understanding the root causes of shifts are crucial steps before implementing ML solutions.
  - The text underscores the complexity of managing data distribution shifts and the necessity of robust monitoring and adaptive strategies to maintain ML model performance in dynamic environments.

##### 8.3. Monitoring and Observability

This section provides a comprehensive overview of monitoring and observability for machine learning (ML) systems in production. It highlights the necessity of monitoring ML systems to ensure their proper functioning and differentiates between monitoring and observability:

- **Monitoring** involves tracking, measuring, and logging various metrics to identify issues.
- **Observability** refers to configuring a system to provide insights for investigating issues, often involving instrumentation like adding timers and logging unusual events. 

Monitoring focuses on tracking predefined metrics and alerting when these metrics breach set thresholds, while observability is about gaining deep insights into the system's internal states by analyzing logs, metrics, and traces to diagnose and understand complex issues.
Short Example:
**Monitoring**
- Scenario: Your e-commerce website tracks server CPU usage.
- Action: An alert is set to notify you if CPU usage exceeds 80%.
- Response: When the alert is triggered, you investigate and find that a high CPU process is running. You restart the process to bring the CPU usage back down.
**Observability**
- Scenario: Customers report that the website is slow during checkout.
- Action: You use observability tools to trace requests, analyze logs, and check detailed metrics.
- Insight: You discover that a new feature release caused database query latency due to inefficient indexing.
- Response: You optimize the query and deploy the fix, improving the checkout performance.



Key concepts include:

**Operational Metrics**:
- Essential for assessing the health of ML systems, these metrics include network latency, CPU/GPU utilization, memory usage, and system uptime.

**ML-Specific Metrics**:
  - Metrics related to model accuracy, predictions, features, and raw inputs.
  - Accuracy metrics, derived from user feedback, are crucial for assessing model performance.
  - Monitoring predictions helps detect distribution shifts and anomalies.
  - Feature monitoring involves validating features against expected schemas and detecting distribution shifts, though it poses challenges like alert fatigue and complexity in processing.

**Monitoring Raw Inputs**:
  - Monitoring raw inputs before processing can help identify data-related issues, though it is often managed by data platform teams.

**Monitoring Toolbox**:
  - Tools used for monitoring include logs, dashboards, and alerts.
  - Logs record runtime events, but managing them can be challenging due to the large volume.
  - Dashboards visualize metrics to make monitoring accessible, though they require expertise to interpret correctly.
  - Alerts notify relevant stakeholders of issues, but must be configured to avoid alert fatigue.

**Observability**:
  - Emphasizes the importance of setting up systems to infer internal states from external outputs.
  - Includes the concept of telemetry (logs and metrics collected at runtime).
  - Observability helps in understanding the complex behavior of ML systems, encompassing interpretability to diagnose issues without deploying new code.

**Challenges in Monitoring**:
  - Monitoring is not straightforward; understanding and interpreting metrics requires statistical knowledge.
  - Detecting performance degradation is only the first step; adapting systems to changing environments is crucial.

