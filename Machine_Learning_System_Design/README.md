### The summary of the book Machine Learning System Design - Huyen Chip

#### 3. Data engineering fundamentals
##### 3.1 Data sources

- ML systems use data from multiple sources with varying characteristics and purposes, requiring different processing methods.
- Understanding data sources enhances efficient data usage in ML systems.
- Key data sources include:
  - **User Input Data:** 
    - Includes text, images, videos, and uploaded files.
    - Prone to malformation and requires thorough validation and fast processing.
  - **System-Generated Data:**
    - Includes logs and system outputs like model predictions.
    - Provides visibility for debugging and improvement.
    - Typically less malformatted and can be processed periodically.
    - Can grow quickly, creating challenges in identifying relevant information and storage.
    - Logging extensively is common for thorough debugging, with services like Logstash, Datadog, and Logz.io aiding in log analysis.
  - **Behavioral Data:**
    - Records user actions (clicks, scrolling, etc.) and is subject to privacy regulations.
  - **Internal Databases:**
    - Generated by services and applications within a company.
    - Used by ML models for tasks like inventory checks and search query processing.
  - **Third-Party Data:**
    - Includes data collected by other companies or public data, often requiring payment.
    - Used for insights like social media activities, purchase history, and demographic analysis.
    - Privacy regulations (like Apple’s IDFA opt-in policy) have reduced availability, pushing companies to rely more on first-party data.
    - Advertisers are developing workarounds, such as the China Advertising Association’s CAID system for device fingerprinting.

##### 3.2 Data formats

- **Data Storage Considerations:**
  - Storing data from multiple sources can be complex and costly.
  - Consider the future use of data to choose an appropriate storage format.
  - Key questions include:
    - How to store multimodal data (images, text).
    - Cost-effective yet fast data storage options.
    - Storing complex models for different hardware compatibility.

- **Data Serialization:**
  - Converts data structures into a storable/transmittable format.
  - Important characteristics of serialization formats: human readability, access patterns, text vs. binary.

- **Common Data Formats:**
  - **JSON:** Text-based, human-readable, widely used, but schema changes are difficult.
  - **CSV:** Text-based, row-major format, better for accessing rows.
  - **Parquet:** Binary, column-major format, better for accessing columns.

- **Row-Major vs. Column-Major Formats:**
  - Row-major (e.g., CSV) is faster for row access and data writes.
  - Column-major (e.g., Parquet) is faster for column-based reads and large datasets.

- **Text vs. Binary Formats:**
  - Text files (e.g., CSV, JSON) are human-readable but larger.
  - Binary files (e.g., Parquet) are more compact and efficient.

- **Relational Data Model:**
  - Organizes data into tables (relations) with rows (tuples).
  - Normalization reduces redundancy but spreads data across tables.
  - SQL is the primary query language, declarative in nature, with query optimization being crucial.

- **Declarative ML Systems:**
  - Inspired by declarative data systems, aim to simplify ML tasks.
  - Examples: Ludwig (Uber), H2O AutoML, which automate model selection and training.

- **NoSQL Data Models:**
  - **Document Model:**
    - Stores data in self-contained documents (e.g., JSON, BSON).
    - Flexible, schemaless but shifts schema assumption to the reading application.
  - **Graph Model:**
    - Focuses on relationships between data items, using nodes and edges.
    - Efficient for relationship-based queries.

- **Structured vs. Unstructured Data:**
  - **Structured Data:**
    - Follows a predefined schema, easier to analyze.
    - Stored in data warehouses(ex : Google BigQuerry, SnowFlake)
  - **Unstructured Data:**
    - Does not follow a schema, more flexible.
    - Stored in data lakes(ex: Amazon S3 (Simple Storage Service) with AWS Lake Formation, Azure Data Lake Storage (ADLS))
  - Data warehouses are for processed data; data lakes store raw data.

##### 3.3 Data Storage Engines and Processing

- **Data Storage Engines and Processing:**
  - Data formats and models define how data is stored/retrieved.
  - Storage engines (databases) implement these processes.
  - Understanding different databases is crucial for application-specific needs.

- **Transactional vs. Analytical Processing:**
  - **Transactional Processing (OLTP):**
    - Handles individual actions (e.g., tweeting, ordering a ride).
    - Requires low latency and high availability.
    - Often uses ACID principles (Atomicity, Consistency, Isolation, Durability).
  - **Analytical Processing (OLAP):**
    - Handles data aggregation and analysis.
    - Efficient for queries involving data from multiple rows/columns.
  - Modern databases often blend OLTP and OLAP capabilities (e.g., CockroachDB, Apache Iceberg, DuckDB).

- **ETL (Extract, Transform, Load) Process:**
  - Essential for ML system data handling.
  - **Extract:**
    - Retrieve data from various sources, validate, and reject corrupted data.
  - **Transform:**
    - Clean, standardize, and process data (joining, deduplicating, aggregating).
  - **Load:**
    - Load transformed data into target destinations (databases, data warehouses).
  - ETL is foundational for data processing in organizations.

- **Evolution of Data Storage:**
  - Growth of data sources and changing schemas led to storing raw data in data lakes.
  - ELT (Extract, Load, Transform) stores data first, processes later, allowing fast data arrival.
  - Challenges with raw data search inefficiency and evolving infrastructures have led to hybrid solutions combining data lakes and warehouses (e.g., Databricks, Snowflake's data lakehouse).

- **Key Concepts:**
  - **Transactional Databases:**
    - Typically row-major, efficient for individual transactions but not for aggregating data.
  - **Analytical Databases:**
    - Efficient for aggregating and analyzing large datasets.
  - **Decoupling Storage from Processing:**
    - Allows storing data in one place with different processing layers for various queries.

- **Terminology:**
  - **ACID vs. BASE:**
    - ACID: Strong consistency and reliability.
    - BASE: More flexibility with eventual consistency.
  - **Online, Nearline, Offline Processing:**
    - Online: Immediate data availability.
    - Nearline: Quick data availability without human intervention.
    - Offline: Requires human intervention for data access.

##### 3.4 Dataflow Modes

- **Dataflow Modes:**
  - **Data Passing Through Databases:**
    - Data is written by one process and read by another from a shared database.
    - Limitations: both processes must access the same database and database read/write speeds may not meet latency requirements.
  
  - **Data Passing Through Services:**
    - Data is exchanged through network requests (e.g., REST, RPC).
    - Used in service-oriented and microservice architectures.
    - Allows independent development and maintenance of different application components.
    - Examples: Ride-sharing app services (driver management, ride management, price optimization).
  
  - **Data Passing Through Real-Time Transport:**
    - Data is broadcast to an intermediary broker rather than directly between services.
    - Reduces complexity and dependency issues in interservice communication.
    - More efficient for data-heavy systems with strict latency requirements.
    - Examples of real-time transport: Apache Kafka, Amazon Kinesis (pubsub), Apache RocketMQ, RabbitMQ (message queue).

- **Request-Driven Architecture:**
Request-driven architecture, often seen in RESTful APIs and microservices architectures, focuses on systems where clients send requests to services, and services respond with the requested data or action. This type of architecture is synchronous, meaning the client waits for the server to process the request and return a response. Here's an example to illustrate request-driven architecture:
    - User Action: A user opens the ShopSmart app and searches for "smartphones."
    - Client Request: The frontend app sends a GET request to the API Gateway endpoint /products?search=smartphones.
    - API Gateway: Forwards the request to the Product Service.
    - Product Service: Queries the database for products matching "smartphones" and returns the list to the API Gateway.
    - Response: The API Gateway sends the product list back to the frontend app, displaying it to the user.
Key features:
    - Services communicate via direct requests.
    - Suitable for logic-heavy systems.
    - Potential issues: complexity in communication, dependency on service availability, and potential bottlenecks.
- **Event-Driven Architecture:**
  - Uses brokers to manage data passing (in-memory storage for real-time transport).
  - Suitable for data-heavy systems.
  - Data broadcast to a real-time transport is called an event; the system is also called an event bus.
  - Pubsub model: Services publish to topics and any subscribing service can read the events.
  - Message queue model: Events (messages) have intended consumers, and the queue ensures delivery to the right consumers.
  
- **Examples and Popular Implementations:**
  - **Pubsub Solutions:**
  In a pub-sub model, message senders (publishers) send messages to a topic, and multiple receivers (subscribers) receive those messages by this topic. This model decouples the producers and consumers of messages, allowing for a more flexible and scalable architecture.
  Use cases example:
  * Publisher: User actions such as posting a status update or liking a photo.
  * Subscribers: Followers of the user who need to be notified of the update
  * Flow:
    - When a user posts a status update, the action is published to a topic.
    - All followers of the user are subscribed to this topic and receive notifications in real-time.  
  Tool example:
    - Apache Kafka
    - Amazon Kinesis
  - **Message Queue Solutions:**
  In a message queue model, messages are stored in a queue and processed by consumers in a first-in, first-out (FIFO) order. This model is designed for point-to-point communication, ensuring that each message is processed by exactly one consumer.
  Use Case Example: Order Processing System in an E-Commerce App:
  * Producer: The order placement service that generates new orders.
  * Consumer: The order fulfillment service that processes and ships orders.
  * Flow:
    - When a customer places an order, the order details are sent to a message queue.
    - The order fulfillment service reads the order from the queue and processes it.
    - The message is removed from the queue once it is successfully processed.
  Another example could be taken is E-cataloging process, when we ask the users validate the result of the model
  Tool example:
    - Apache RocketMQ
    - RabbitMQ
  - **The main difference PubSub and Mesage Queue**
    - Pub-Sub Solutions: Ideal for scenarios where messages need to be broadcast to multiple subscribers in real-time, such as notifications or real-time updates.
    - Message Queue Solutions: Best suited for scenarios requiring reliable, point-to-point communication with message persistence and load balancing, such as task processing or order fulfillment.

- **Advantages of Real-Time Transport:**
  - Simplifies communication between services.
  - Decouples service dependencies.
  - Enhances scalability and resilience.
  - Ensures timely data availability for applications with strict latency requirements.

##### 3.5 Batch Processing Versus Stream Processing 
- **Historical Data vs. Streaming Data:**
  - **Historical Data:** Stored in data storage engines (databases, data lakes, data warehouses) and processed in batch jobs.
  - **Streaming Data:** Continuously flows through real-time transports like Apache Kafka and Amazon Kinesis, processed in real-time.

- **Batch Processing:**
  - Processes historical data periodically (e.g., daily).
  - Efficient for static features (e.g., driver ratings).
  - Uses distributed systems like MapReduce and Spark.
  - Suitable for features that change less frequently.
  - Known as static features.

- **Stream Processing:**
  - Processes streaming data in real-time or in short intervals (e.g., every five minutes).
  - Ideal for dynamic features (e.g., current number of available drivers).
  - Low latency as it processes data as soon as it is generated.
  - Uses scalable technologies like Apache Flink, KSQL, and Spark Streaming.
  - Suitable for features that change quickly.
  - Known as dynamic features.

- **Advantages of Stream Processing:**
  - Provides low latency by processing data immediately.
  - Efficient for stateful computation, preventing redundancy.
  - Highly scalable and capable of parallel computation.
  - Suitable for applications requiring frequent updates, such as fraud detection and credit scoring.

- **Challenges in Stream Processing:**
  - Handling unbounded data with variable rates and speeds.
  - More complex than batch processing due to continuous data flow.

- **Combining Batch and Stream Processing:**
  - Many problems require both static (batch) and dynamic (streaming) features.
  - Infrastructure needed to process and join both types of data for ML models.
  - Apache Flink and KSQL provide SQL abstraction, useful for complex stream feature extraction.

- **Stream Computation Engines:**
  - **Apache Kafka:** Built-in stream computation capabilities, limited with various data sources.
  - **Apache Flink and KSQL:** Recognized for efficient stream processing with SQL abstraction.
  - **Spark Streaming:** Another option for stream processing.

- **Conclusion:**
  - Batch processing is simpler but less frequent and suited for static data.
  - Stream processing is complex, more frequent, and suited for dynamic data.
  - Combining both processing types enhances the capability to generate accurate ML model predictions.

#### 4. Training data

##### 4.1 Sampling

- **Importance of Sampling in ML:**
  - Integral to various stages of ML projects: creating training data, data splitting, and monitoring.
  - Necessary when all real-world data isn’t available or when processing all data is infeasible.
  - Sampling methods improve efficiency and help avoid biases.

- **Types of Sampling:**
  - **Nonprobability Sampling:**
    - Convenience sampling: Based on data availability.
    - Snowball sampling: Future samples selected based on existing samples.
    - Judgment sampling: Experts select samples.
    - Quota sampling: Samples based on quotas without randomization.
    - Often biased and not representative of real-world data.
    - Common in initial data gathering for projects.

  - **Probability-Based Sampling:**
    - **Simple Random Sampling:**
      - Equal probability for all samples.
      - Easy to implement but may miss rare categories.
    - **Stratified Sampling:**
      - Divides population into groups (strata) and samples from each.
      - Ensures representation of all groups but can be challenging for multilabel tasks.
    - **Weighted Sampling:**
      - Samples assigned weights to influence selection probability.
      - Useful for domain expertise and adjusting for distribution differences.
      - Related to sample weights in ML for influencing model training.
    - **Reservoir Sampling:**
      - Useful for streaming data.
      - Ensures equal probability for each sample even when the total data size is unknown.
      - Algorithm maintains a reservoir of samples with correct probabilities.
    - **Importance Sampling:**
      - Samples from an easier distribution (Q) and weights them to match the target distribution (P).
      - Common in policy-based reinforcement learning to update policies efficiently.

- **Nonprobability Sampling Examples:**
  - Language modeling often uses easily collected data (e.g., Wikipedia, Reddit).
  - Sentiment analysis uses naturally labeled data (e.g., IMDB, Amazon reviews).
  - Self-driving cars initially collected data from limited geographic areas.

- **Simple Random Sampling Example:**
  - Randomly selecting 10% of a population ensures equal chances but may miss rare categories.

- **Stratified Sampling Example:**
  - Sampling 1% from two classes ensures both are represented, useful in unbalanced datasets.

- **Weighted Sampling Example:**
  - Assigning higher weights to more recent data or underrepresented categories to reflect real-world distribution.

- **Reservoir Sampling Example:**
  - Sampling tweets from an incoming stream without knowing the total number of tweets.

- **Importance Sampling Example:**
  - In reinforcement learning, using old policy rewards as a proposal distribution for new policy updates.

Understanding and choosing the appropriate sampling method is crucial for creating reliable and efficient ML models.


##### 4.2 Labelling
The text discusses the challenges and methodologies associated with obtaining labeled data for machine learning (ML) models. Despite the promise of unsupervised ML, most models today rely on supervised learning, requiring high-quality labeled data for effective training. Key points include:

**Importance of Labeled Data**:
  - Performance of ML models depends on the quality and quantity of labeled data.
  - Data labeling is now a core function of many ML teams.

**Hand Labeling**:
  - Expensive and time-consuming, especially when expertise is required (e.g., radiologists for medical data).
  - Raises privacy concerns, as data must be shared with annotators.
  - Slow process, leading to slower model adaptation to changes.

**Label Multiplicity**:
  - Different annotators often provide conflicting labels for the same data, causing label ambiguity.
  - Clear problem definitions and consistent training for annotators can reduce disagreements.

**Data Lineage**:
  - Tracking the origin and quality of labeled data is crucial to avoid model performance issues when integrating new data.

**Natural Labels**:
  - Some tasks have natural ground truth labels inferred from system interactions (e.g., Google Maps ETA, stock price predictions, recommender systems).
  - Feedback loop length varies; shorter loops allow quicker model adaptation, while longer loops are common in tasks like fraud detection.

**Handling Lack of Labels**:
  - **Weak Supervision**: Uses heuristics and labeling functions to generate noisy but useful labels. Tools like Snorkel aid this process.
  - **Semi-Supervision**: Combines small amounts of labeled data with assumptions to generate more labels.
  - **Transfer Learning**: Uses pretrained models as a starting point for new tasks, reducing the need for large amounts of labeled data.
  - **Active Learning**: Focuses on labeling the most informative data samples to improve model efficiency with fewer labels.

**Challenges and Strategies**:
  - Addressing hand labeling issues involves cost, privacy, and speed considerations.
  - Techniques like weak supervision, semi-supervision, transfer learning, and active learning help mitigate the lack of labeled data.
  - Active learning is particularly promising for real-time data adaptation.

Overall, the text emphasizes the critical role of data labeling in ML, the challenges faced in obtaining labeled data, and various strategies to overcome these challenges to improve model performance.


##### 4.3 Class imbalance

Class imbalance occurs when the number of samples in each class of training data is significantly different. For instance, in a lung cancer detection dataset, 99.99% of X-rays might be normal, while only 0.01% show cancerous cells. This imbalance can also occur in regression tasks, such as predicting healthcare bills, where extreme values are rare but significant.

**Challenges**
1. Insufficient Signal: Models may not learn to detect minority classes due to the limited number of examples, leading to few-shot learning problems or models that assume rare classes don't exist.
2. Nonoptimal Solutions: Models might exploit simple heuristics (e.g., always predicting the majority class), which gradient descent algorithms struggle to beat.
3. Asymmetric Costs: Errors on rare class samples often have higher costs than errors on majority class samples, making it critical to adjust loss functions accordingly.
**Handling Class Imbalance**
1. Choosing the Right Metrics:
- Accuracy and error rate are insufficient for imbalanced data as they favor the majority class.
- Metrics like precision, recall, F1 score, and area under the ROC curve are better suited to evaluate performance on imbalanced datasets.
2. Data-Level Methods (Resampling):
- Oversampling: Adding more instances of minority classes.
- Undersampling: Removing instances of majority classes.
- Techniques like SMOTE (Synthetic Minority Oversampling Technique) and Tomek links help balance the data but may have limitations in high-dimensional data.
3. Algorithm-Level Methods:
- Cost-Sensitive Learning: Adjusts the loss function to account for different misclassification costs.
- Class-Balanced Loss: Weights each class inversely proportional to its frequency, punishing the model more for errors on minority classes.
- Focal Loss: Focuses on hard-to-classify samples by increasing their weight in the loss function.
**Real-World Applications**
Class imbalance is common in real-world scenarios such as fraud detection, churn prediction, disease screening, and object detection. Handling class imbalance effectively is crucial for building robust machine learning systems that perform well on both majority and minority classes.

##### 4.4 Data augmentation

Data augmentation is a set of techniques used to increase the amount of training data by creating new samples from existing ones. Initially used for tasks with limited data, like medical imaging, these techniques have proven beneficial even with large datasets, enhancing model robustness to noise and adversarial attacks.

**Types of Data Augmentation**
1. Simple Label-Preserving Transformations:
- Computer Vision: Techniques include cropping, flipping, rotating, inverting, and erasing parts of images. These transformations maintain the original labels (e.g., a rotated image of a dog is still labeled as a dog). This method effectively doubles or triples the training data.
- NLP: Involves replacing words with synonyms to preserve the sentence's meaning or sentiment. For example, "I’m so happy to see you" can be transformed into "I’m so glad to see you" or "I’m very happy to see you" using dictionaries or word embeddings.
2. Perturbation:
- Computer Vision: Adding noise to images can help models recognize weak spots in their decision boundaries, improving performance. Techniques include random noise addition or adversarial augmentation (e.g., changing one pixel to misclassify an image).
- NLP: Less common, but can involve replacing random words to improve robustness. For instance, in BERT, 15% of tokens are randomly replaced to create noise, slightly boosting model performance.
3. Data Synthesis:
- NLP: Templates are used to generate training data. For example, the template "Find me a [CUISINE] restaurant within [NUMBER] miles of [LOCATION]" can create numerous queries by filling in the placeholders.
- Computer Vision: Techniques like mixup combine existing examples to create new ones with continuous labels, improving model generalization and robustness. Neural networks can also generate training data, as seen in CycleGAN applications, which have enhanced performance in tasks like CT segmentation.

#### 5. Feature engineering

##### 5.1 Learned Features Versus Engineered Features

**Importance and Current Status**
Feature engineering remains a critical aspect of machine learning, despite the advancements in deep learning. While deep learning, often termed feature learning, promises to reduce the need for handcrafted features by automatically learning and extracting many features, we are not yet at a point where all features can be automated. Moreover, many current ML applications in production do not utilize deep learning.

**Example: Sentiment Analysis Classifier**
- Before Deep Learning: Manual application of text processing techniques like lemmatization, expanding contractions, removing punctuation, lowercasing, and splitting text into n-grams.
  * N-grams: Contiguous sequences of n items from text (e.g., words or syllables). For "I like food", 1-grams are ["I", "like", "food"], and 2-grams are ["I like", "like food"].
  * Feature Vector Creation: Mapping n-grams to indices to create vectors representing each post, which are then used as inputs to ML models.
- Challenges: The iterative and brittle nature of this process often required restarting due to forgotten techniques or poorly performing techniques.
**Deep Learning Advancements**
- Text Processing: Instead of complex preprocessing, raw text is tokenized into words, creating a vocabulary and converting words into one-shot vectors. The model learns to extract useful features.
- Image Processing: Similar progress allows direct input of raw images into deep learning models, eliminating the need for manual feature extraction.
**Beyond Text and Images**
- Additional Data: For tasks like spam detection, additional information beyond text may be needed, such as:
  * Comment Metrics: Upvotes/downvotes.
  * User Metrics: Account age, posting frequency, and upvote/downvote history.
  * Thread Metrics: Views, with popular threads attracting more spam.
- Feature Engineering Process: Involves selecting relevant information and converting it into a usable format for ML models. This can include millions of features for complex tasks like video recommendation systems on platforms like TikTok. Domain-specific tasks, like fraud detection, require subject matter expertise to identify useful features

##### 5.2 Common Feature Engineering Operations


#### 6. Model Development and Offline Evaluation
[to be continued ]


#### 7. Model Deployment and Prediction Service

##### 7.1 Batch Prediction Versus Online Prediction
The text provides an in-depth discussion on the differences and implications of using batch prediction versus online prediction in machine learning (ML) systems. Key points include:

**Prediction Modes**:
  - **Batch Prediction**: Generates predictions periodically or when triggered, storing them for future use. Known as asynchronous prediction.
  - **Online Prediction**: Generates predictions in real-time as requests arrive, typically via RESTful APIs. Known as synchronous prediction.
  - **Streaming Prediction**: A type of online prediction that uses both batch and streaming features.

**Batch vs. Online Prediction**:
  - **Batch Prediction**:
    - Uses historical (batch) data.
    - Suitable for scenarios where immediate results are not required (e.g., periodic movie recommendations).
    - Optimized for high throughput but less responsive to real-time changes in user preferences.
    - Can reduce inference latency for complex models by precomputing predictions.
  - **Online Prediction**:
    - Uses real-time (streaming) data and batch data.
    - Necessary for applications requiring immediate results (e.g., fraud detection, autonomous vehicles).
    - Optimized for low latency but traditionally considered less efficient in terms of cost and performance.

**Terminology Confusion**:
  - Terms like “online prediction” and “batch prediction” can be confusing. The text suggests using “synchronous prediction” and “asynchronous prediction” but acknowledges that even these terms are not perfect.

**Use Cases**:
  - **Batch Prediction**: Used for generating large volumes of predictions, such as recommendations for all users periodically.
  - **Online Prediction**: Used for generating predictions on-demand, essential for applications like real-time language translation and high-frequency trading.

**Hybrid Solutions**:
  - Some systems precompute predictions for popular queries (batch prediction) and generate predictions online for less popular queries, combining the benefits of both approaches.

**Transition from Batch to Online Prediction**:
  - Online prediction is often the first approach used during prototyping due to its immediate feedback nature.
  - Batch prediction can be a workaround for scenarios where online prediction is too slow or costly.
  - As hardware and techniques improve, the trend is moving towards more online predictions.

**Challenges and Infrastructure**:
  - Building a unified pipeline for both batch and streaming data is complex but essential for reducing bugs and maintaining consistency.
  - Companies are investing in stream processing frameworks and feature stores to achieve this unification.

**Key Differences**:
  - Batch prediction processes accumulated data periodically, useful for recommendations.
  - Online prediction processes data as soon as it arrives, critical for real-time applications.

##### 7.2 Model compression

The text explores various techniques for reducing the inference latency of machine learning (ML) models, emphasizing model compression and optimization. Here are the key points:

**Inference Optimization Approaches**:
  - **Make inference faster**: Improve the efficiency of the inference process.
  - **Make the model smaller**: Reduce the model size through compression techniques.
  - **Improve hardware performance**: Use faster hardware for deploying the model.

**Model Compression**:
  - **Low-Rank Factorization**: Replaces high-dimensional tensors with lower-dimensional ones to reduce parameters and increase speed. Examples include compact convolutional filters and MobileNets.
  - **Knowledge Distillation**: A smaller model (student) is trained to mimic a larger model (teacher), retaining much of the teacher's capabilities but with reduced size and faster inference. Example: DistilBERT.
  - **Pruning**: Removes less important parameters from the model, making it sparser and more efficient without significantly compromising accuracy. This can involve setting certain parameters to zero or removing entire nodes.
  - **Quantization**: Reduces the number of bits used to represent model parameters, thus decreasing memory usage and improving computation speed. Commonly involves using lower precision (e.g., 16-bit or 8-bit integers) instead of the default 32-bit floats.

**Detailed Techniques**:
  - **Low-Rank Factorization**: Used mainly for convolutional neural networks, it involves strategies like replacing 3x3 convolutions with 1x1 convolutions.
  - **Knowledge Distillation**: Useful for deploying smaller models, it can work across different architectures but requires an existing larger model as a teacher.
  - **Pruning**: Can significantly reduce the nonzero parameters of a neural network, making it more efficient.
  - **Quantization**: General and widely used, it reduces the precision of model parameters, resulting in faster computations and reduced memory footprint. It can be applied during or after training.

**Quantization Details**:
  - **Half Precision (16-bit)**: Reduces memory usage by half compared to 32-bit.
  - **Fixed Point (8-bit)**: Further reduces memory usage and improves speed. Binary weight neural networks represent an extreme case with 1-bit weights.
  - **Trade-offs**: Lower precision can lead to rounding errors and under/overflow issues, which need careful handling.

**Case Study - Roblox**:
  - Roblox scaled BERT to handle over 1 billion daily requests on CPUs by implementing various compression techniques.
  - They started with a large BERT model, then used DistilBERT and dynamic shape input, and finally quantized the model.
  - Quantization provided the most significant performance boost, reducing latency sevenfold and increasing throughput eightfold.

##### 7.3 ML on the Cloud and on the Edge

The text discusses the considerations and trade-offs between deploying machine learning (ML) models on the cloud versus on edge devices. 

### Key Points:

**Cloud vs. Edge Computation**:
  - **Cloud Computation**: Involves performing a large portion of computations on public or private cloud servers.
  - **Edge Computation**: Involves performing computations on consumer devices (e.g., phones, laptops, cars, smartwatches).

**Cloud Deployment**:
  - **Ease of Use**: Managed cloud services (AWS, GCP) make it straightforward to deploy ML models.
  - **Cost Concerns**: Cloud computations can be expensive. Large companies spend millions annually, and small/medium companies also incur significant costs. Mishandling cloud services can financially strain startups.

**Edge Deployment**:
  - **Cost Efficiency**: Reduces the amount of computation required on the cloud, thereby lowering costs.
  - **Operational Benefits**:
    - Works without internet or with unreliable connections, making it viable in remote areas or places with strict no-internet policies.
    - Reduces network latency as computations occur locally on the device, improving real-time performance.
  - **Privacy and Security**: Keeps user data on local devices, mitigating risks of data breaches and aiding compliance with data regulations (e.g., GDPR). However, physical device theft remains a risk.
  - **Device Requirements**: Edge devices need sufficient computational power, memory, and battery life to handle ML models.

**Trends and Developments**:
  - **Rise of Edge Devices**: Companies are developing specialized hardware for ML use cases (e.g., Google, Apple, Tesla), and the number of active edge devices is projected to exceed 30 billion by 2025.
  - **Optimization Challenges**: Running ML models efficiently on diverse hardware requires significant effort. Intermediate representations (IRs) and compilers help bridge frameworks and hardware platforms, simplifying this process.

**Model Optimization**:
  - **Compilers and IRs**: Translate high-level model code into machine code specific to hardware backends. This involves optimizing computation graphs for efficiency.
  - **Techniques**: Include vectorization, parallelization, loop tiling, and operator fusion to enhance performance.
  - **ML for Optimization**: Tools like autoTVM use ML to optimize models by predicting the best execution paths, though this process can be time-consuming.
In the end, after optimizing model for specific type of GPU, you can have a model which is faster for the inference.

**ML in Browsers**:
  - **WebAssembly (WASM)**: Allows running ML models in browsers, making them hardware-agnostic. While WASM is faster than JavaScript, it is still slower than native applications.


#### 8. Data distribution shifts and monitoring
A company deployed an ML model to predict grocery demand, initially achieving good results. Over time, the model's accuracy declined, leading to inventory mismanagement.
The company faced expensive options to update the model or build an in-house team. Lession leanned:
- Deploying an ML model requires ongoing monitoring and updates to maintain performance.
- Changes in data patterns over time can cause models to fail, highlighting the need for regular updates.

##### 8.1. Causes of ML System FailuresData

- Failure: Occurs when one or more system expectations are violated.
- Operational Expectations: Similar to traditional software, focusing on metrics like latency and throughput.
- ML Performance Expectations: Unique to ML systems, focusing on the accuracy and quality of predictions.

**Types of Failures**
- Software System Failures: 
  - Dependency Failure: Breaks due to third-party dependencies.
  - Deployment Failure: Errors during deployment, such as incorrect binaries or permissions issues.
  - Hardware Failures: Failures due to malfunctioning hardware like CPUs or GPUs.
  - Downtime or Crashing: System failures due to server downtimes.
  - Predominantly non-ML related but essential for ML engineers to address using traditional software engineering skills.
  - Example: Google’s ML pipeline failures often stemmed from distributed system or data pipeline issues.

- ML-Specific Failures:
  - Data Collection and Processing Problems: Issues arising from incorrect or inadequate data.
  - Poor Hyperparameters: Suboptimal settings leading to poor model performance.
  - Training-Inference Pipeline Discrepancies: Mismatches between training and inference environments.
  - Data Distribution Shifts: Changes in data over time that the model was not trained to handle.
  - Edge Cases: Rare, extreme cases that the model cannot handle well.
  - Degenerate Feedback Loops: Situations where model outputs influence future inputs, leading to biased or suboptimal performance.

#### Detailed ML-Specific Failures
- Production Data Differing from Training Data:
  - Generalization Issue: Training data might not represent the real-world data accurately.
  - Real-World Data Variability: Real-world data can shift over time, making initial training data insufficient.
  - Examples: Shifts due to events like COVID-19, seasonal changes, or internal errors like bugs in the data pipeline.

- Edge Cases:
  - High-Risk Scenarios: ML models failing in critical scenarios (e.g., self-driving cars, medical diagnosis).
  - Outliers vs. Edge Cases: Outliers are data points significantly different from others, while edge cases are where the model performs poorly on a rare case, but it have big effect.

- Degenerate Feedback Loops:
  - Feedback Influence: When model predictions affect user interactions, which in turn affect future predictions.
  - Common in Recommender Systems: Items getting more exposure due to initial high ranking, leading to popularity bias.
  - Examples in recommeding ads: some ads keep showing ups in the website just because the AI thinks it's the most interesting for the users, and the users keep clicking it just because they don't see anything else show, which make the AI even more belive that its algorithm is correct.

- Detecting and Correcting Degenerate Feedback Loops
  - Detection: Measure diversity and popularity of system outputs; observe if predictions become homogeneous over time.
  - Correction Methods:
      - Randomization: Introducing randomness in predictions to reduce homogeneity.
      - Positional Features: Using features that account for the position of an item in a recommendation list to mitigate bias.
      - Two-Model Approach: Separate models for predicting visibility and interaction likelihood. First model to predict the probability that a user will see and consider an ad based on its position. The second model that predict the probability that a user will click on an ad given that they have seen and considered it.In the end, we rank the ads based on the combined probabilities from both models.

##### 8.2. Data distribution shifts

The text provides an in-depth analysis of data distribution shifts in supervised machine learning (ML) systems, which occur when the data a model is trained on differs from the data it encounters in production, causing a decline in predictive accuracy over time. It emphasizes the importance of understanding and addressing these shifts to maintain model performance.

### Key Points:

**Definition and Types of Data Distribution Shifts**:
  - **Covariate Shift**: Change in the distribution of the input data (\(P(X)\)) while the relationship between inputs and outputs (\(P(Y|X)\)) remains the same.
  - **Label Shift**: Change in the distribution of the output data (\(P(Y)\)) while the relationship between outputs and inputs (\(P(X|Y)\)) remains the same.
  - **Concept Drift**: Change in the relationship between inputs and outputs (\(P(Y|X)\)) while the input distribution (\(P(X)\)) remains the same.

**Causes and Examples**:
  - Covariate shifts can arise from biases in data collection, changes in the environment, or artificial data alterations for model training.
  - Label shifts often occur alongside covariate shifts but can happen independently, such as when a preventive measure changes disease incidence rates.
  - Concept drift can result from significant events (e.g., COVID-19 affecting housing prices) or cyclic/seasonal variations.

**Detection of Data Shifts**:
  - Monitoring accuracy-related metrics in production is crucial but challenging due to the lack of real-time ground truth labels.
  - Statistical methods and two-sample tests (e.g., Kolmogorov-Smirnov test) can help detect shifts by comparing training and production data distributions.
  - Importance of time scale windows in detecting temporal shifts, with cumulative and sliding statistics aiding in monitoring.

**Addressing Data Shifts**:
  - Regular retraining of models using updated data to adapt to new distributions.
  - Using massive datasets to train robust models that generalize well to new data.
  - Advanced techniques like domain adaptation and transfer learning for model adjustment without extensive retraining.
  - Designing systems to be more resilient to shifts by carefully selecting and managing features and employing separate models for different segments.

**Practical Considerations**:
  - Companies must balance model performance and feature stability to minimize frequent retraining.
  - Addressing human errors and understanding the root causes of shifts are crucial steps before implementing ML solutions.
  - The text underscores the complexity of managing data distribution shifts and the necessity of robust monitoring and adaptive strategies to maintain ML model performance in dynamic environments.

##### 8.3. Monitoring and Observability

This section provides a comprehensive overview of monitoring and observability for machine learning (ML) systems in production. It highlights the necessity of monitoring ML systems to ensure their proper functioning and differentiates between monitoring and observability:

- **Monitoring** involves tracking, measuring, and logging various metrics to identify issues.
- **Observability** refers to configuring a system to provide insights for investigating issues, often involving instrumentation like adding timers and logging unusual events. 

Monitoring focuses on tracking predefined metrics and alerting when these metrics breach set thresholds, while observability is about gaining deep insights into the system's internal states by analyzing logs, metrics, and traces to diagnose and understand complex issues.
Short Example:
**Monitoring**
- Scenario: Your e-commerce website tracks server CPU usage.
- Action: An alert is set to notify you if CPU usage exceeds 80%.
- Response: When the alert is triggered, you investigate and find that a high CPU process is running. You restart the process to bring the CPU usage back down.
**Observability**
- Scenario: Customers report that the website is slow during checkout.
- Action: You use observability tools to trace requests, analyze logs, and check detailed metrics.
- Insight: You discover that a new feature release caused database query latency due to inefficient indexing.
- Response: You optimize the query and deploy the fix, improving the checkout performance.



Key concepts include:

**Operational Metrics**:
- Essential for assessing the health of ML systems, these metrics include network latency, CPU/GPU utilization, memory usage, and system uptime.

**ML-Specific Metrics**:
  - Metrics related to model accuracy, predictions, features, and raw inputs.
  - Accuracy metrics, derived from user feedback, are crucial for assessing model performance.
  - Monitoring predictions helps detect distribution shifts and anomalies.
  - Feature monitoring involves validating features against expected schemas and detecting distribution shifts, though it poses challenges like alert fatigue and complexity in processing.

**Monitoring Raw Inputs**:
  - Monitoring raw inputs before processing can help identify data-related issues, though it is often managed by data platform teams.

**Monitoring Toolbox**:
  - Tools used for monitoring include logs, dashboards, and alerts.
  - Logs record runtime events, but managing them can be challenging due to the large volume.
  - Dashboards visualize metrics to make monitoring accessible, though they require expertise to interpret correctly.
  - Alerts notify relevant stakeholders of issues, but must be configured to avoid alert fatigue.

**Observability**:
  - Emphasizes the importance of setting up systems to infer internal states from external outputs.
  - Includes the concept of telemetry (logs and metrics collected at runtime).
  - Observability helps in understanding the complex behavior of ML systems, encompassing interpretability to diagnose issues without deploying new code.

**Challenges in Monitoring**:
  - Monitoring is not straightforward; understanding and interpreting metrics requires statistical knowledge.
  - Detecting performance degradation is only the first step; adapting systems to changing environments is crucial.


#### 9. Continual Learning and Test in Production

##### 9.1. Continual learning

Continual learning involves updating a model periodically with new data. Unlike constant retraining with each incoming sample, which is rare due to the risk of catastrophic forgetting and high computational costs, continual learning typically uses micro-batches (e.g., updating after every 512 or 1,024 examples).

**Model Update Process:**
- Instead of updating the existing model directly, a replica (challenger) is created and updated. If this updated model performs better, it replaces the existing model (champion). Companies may use multiple challengers simultaneously.

**Retraining Frequency:**
- The frequency of model updates should be task-dependent, based on factors like data traffic and model decay rate. Many companies don't need very frequent updates (e.g., every 5 or 10 minutes) unless there's sufficient new data or the model decays rapidly.

**Stateless vs. Stateful Training:**
- **Stateless Retraining:** The model is retrained from scratch each time.
- **Stateful Training:** The model continues training from its last state, requiring less data and computing power. Stateful training allows for updates with fresh data only and can reduce storage needs by not requiring permanent data storage.

**Benefits of Stateful Training:**
- Faster convergence and reduced compute costs.
- Eliminates the need to store data permanently, alleviating privacy concerns.

**Challenges and Best Practices:**
- Infrastructure setup for continual learning should allow for both stateless and stateful retraining.
- The training frequency should be optimized based on the specific needs and performance gains from fresher data.
- Continual learning is essential for handling data distribution shifts and adapting to rare events, such as sudden surges in demand or special shopping events.

**Examples of Use Cases:**
- **Ride-sharing Pricing:** Quickly adapting to sudden changes in demand.
- **E-commerce:** Adapting recommendations during events like Black Friday.
- **Cold Start Problem:** Handling new users or infrequent visitors by quickly adapting recommendations based on their session behavior.

**Continual Learning Challenges:**
- **Fresh Data Access:** Ensuring timely access to new data, possibly through real-time data streams.
- **Evaluation:** Thoroughly testing updates to avoid performance degradation and ensuring robustness against adversarial attacks.
- **Algorithm Suitability:** Some algorithms (e.g., neural networks) adapt better to continual learning than others (e.g., matrix-based models).

**Stages of Continual Learning:**
1. **Manual, Stateless Retraining:** Initial stage with ad-hoc updates.
2. **Automated Retraining:** Periodic updates using automated scripts.
3. **Automated, Stateful Training:** Implementing stateful training to reduce retraining costs.
4. **Continual Learning:** Dynamic updates triggered by data distribution shifts or performance changes.

**Future of Continual Learning:**
- As MLOps tooling matures, continual learning could become as straightforward as batch learning, enabling more adaptive and responsive machine learning models.

**Conclusion:**
- Continual learning enhances model performance by keeping them up-to-date with the latest data, addressing issues like data distribution shifts, rare events, and the cold start problem. It requires robust infrastructure, effective evaluation mechanisms, and the right balance between stateless and stateful training.

##### 9.2. Test in production

### Summary of Model Evaluation and Deployment Strategies

**Model Evaluation Methods:**
- **Test Splits:** Static benchmarks to compare multiple models, but they may not reflect new data distributions. 
- **Backtests:** Evaluate models on recent data to reflect current distributions but should be supplemented with static test splits for reliability.

**Deployment Techniques:**
1. **Shadow Deployment:**
   - Deploy the candidate model alongside the existing model.
   - Route requests to both models but serve only the existing model's predictions.
   - Log the new model's predictions for analysis.
   - Advantage: Low risk as the new model's predictions are not served.
   - Disadvantage: Doubles inference compute costs.

2. **A/B Testing:**
   - Compare the existing model with a candidate model by routing a percentage of traffic to each.
   - Ensure randomized traffic routing and sufficient sample size.
   - Measure statistical significance to determine which model performs better.
   - Useful for determining the better model but requires careful design and execution.

3. **Canary Release:**
   - Gradually roll out the candidate model to a subset of users.
   - Increase traffic to the candidate model if performance is satisfactory; otherwise, revert to the existing model.
   - Can be used independently or in conjunction with A/B testing.

4. **Interleaving Experiments:**
   - Show recommendations from both models to users and measure which model’s recommendations are preferred.
   - Effective for ranking algorithms with smaller sample sizes compared to A/B testing.
   - Ensure equal probability for recommendations from each model to maintain validity.

5. **Bandit Algorithms:**
   - Balance between exploitation (using the best-performing model) and exploration (testing other models).
   - More data-efficient and can quickly determine the best model with fewer samples.
   - Examples: ε-greedy, Thompson Sampling, Upper Confidence Bound (UCB).

**Challenges and Considerations:**
- **Algorithm Suitability:** Bandits are more suitable for tasks with short feedback loops and online predictions.
- **Implementation Complexity:** Bandits are harder to implement due to the need for real-time performance tracking and feedback mechanisms.
- **Contextual Bandits:** Extend bandit algorithms to determine the payout of actions (e.g., ad recommendations) and balance exploration-exploitation trade-offs.
  
**Best Practices:**
- **Automated Evaluation Pipelines:** Define clear evaluation pipelines to mitigate biases and ensure consistency. Automate these pipelines for continuous integration and deployment.
- **Proper Sampling and Randomization:** Ensure A/B testing and other evaluation methods are properly randomized and have sufficient samples for valid results.
- **Thorough Testing:** Evaluate models extensively before deployment to avoid failures and mitigate risks of adversarial attacks.

Overall, combining offline and online evaluation methods, and using various deployment techniques such as shadow deployment, A/B testing, canary releases, interleaving experiments, and bandit algorithms, can ensure models are sufficiently evaluated and safely deployed.

#### 10. Infrastructure and tooling for MLops

##### 10.1 Development Environment

The development environment for machine learning (ML) engineers consists of the Integrated Development Environment (IDE), versioning tools, and Continuous Integration/Continuous Deployment (CI/CD) systems. Despite its importance, the dev environment is often underrated in many companies, leading to ad-hoc solutions for code development, debugging, and testing.

- **IDE**: Tools like VS Code, Vim, and browser-based IDEs such as AWS Cloud9 are commonly used. Data scientists also heavily utilize notebooks like Jupyter and Google Colab for their exploratory capabilities.
- **Versioning**: Tools like Git, DVC, Weights & Biases, Comet.ml, and MLflow are used for versioning code, data, and model artifacts.
- **CI/CD**: GitHub Actions and CircleCI are examples of tools used to automate testing and deployment processes.

#### Importance of Standardizing Dev Environments
Standardizing the development environment, either company-wide or team-wide, can significantly improve engineering productivity and consistency. The text describes challenges faced when using inconsistent development setups and the eventual move to cloud-based environments for standardization.

- **Local vs. Cloud**: Moving to cloud dev environments simplifies IT support, enhances remote work capabilities, and improves security.
- **Cloud IDEs**: Tools like GitHub Codespaces and AWS EC2 instances are used to provide standardized cloud environments.

#### Transition to Production with Containers
Containers, particularly Docker, are used to ensure that code runs consistently across different environments by packaging the application along with its dependencies. This is crucial for moving from development to production where workloads can be dynamic.

- **Docker**: Containers are created from Docker images, which are defined by Dockerfiles. These images ensure that the environment setup is consistent and replicable across multiple instances.
- **Container Orchestration**: Kubernetes (K8s) is used for managing large numbers of containers across different hosts, providing features like autoscaling, load balancing, and high availability.

#### Example of Docker Usage
An example Dockerfile is provided, illustrating the steps to create a Docker image for a project involving PyTorch and Hugging Face's transformers library.

```yaml
FROM pytorch/pytorch:latest
RUN git clone https://github.com/NVIDIA/apex
RUN cd apex && \
    python3 setup.py install && \
    pip install -v --no-cache-dir --global-option="--cpp_ext" \
    --global-option="--cuda_ext" ./
WORKDIR /fancy-nlp-project
RUN git clone https://github.com/huggingface/transformers.git && \
    cd transformers && \
    python3 -m pip install --no-cache-dir .
```

This file shows how to set up the environment by installing necessary packages and setting up directories.

#### Multi-Container and Orchestration
For complex applications requiring multiple containers, container orchestration tools like Docker Compose (for single host) and Kubernetes (for multiple hosts) are essential. These tools manage the lifecycle of containers, ensuring they are correctly deployed, scaled, and maintained.

- **Kubernetes (K8s)**: Offers advanced features for managing containerized applications across clusters, maintaining service availability, and optimizing resource usage.

By leveraging these tools and practices, organizations can create robust, scalable, and maintainable ML systems, bridging the gap between development and production environments effectively.

##### 10.2 Resource management


**Pre-Cloud vs. Cloud Resource Management**
- **Pre-Cloud**: Resource management involved maximizing limited compute and storage resources within data centers, often requiring complex logic and significant engineering time.
- **Cloud**: Resources are elastic, shifting focus to cost-effective usage. Companies prefer investing in automation to free up engineers for higher-value tasks, even if it results in less efficient resource usage.

**Characteristics of ML Workflows**
- **Repetitiveness**: ML workflows often involve repetitive tasks, such as regular model training or batch predictions.
- **Dependencies**: ML workflow steps often depend on the success of previous steps, forming complex dependency relationships.

**Scheduling and Orchestration**
- **Cron Jobs**: Used for scheduling repetitive tasks at fixed times, but lacks handling of dependencies.
- **Schedulers**: Advanced cron-like systems that manage dependencies and resources, often represented as Directed Acyclic Graphs (DAGs).
- **Orchestrators**: Manage where resources come from and ensure the necessary compute resources are available, often integrating with schedulers.

Example of a Scheduler: Slurm
```bash
#!/bin/bash
#SBATCH -J JobName
#SBATCH --time=11:00:00 # When to start the job
#SBATCH --mem-per-cpu=4096 # Memory, in MB, to be allocated per CPU
#SBATCH --cpus-per-task=4 # Number of cores per task
```

**Workflow Management Tools**
- **Airflow**: Early task scheduler with many operators but suffers from monolithic design and static, non-parameterized DAGs.
- **Prefect**: Dynamic and parameterized workflows, addressing many drawbacks of Airflow but with less focus on containerization.
- **Argo**: Emphasizes containerized steps with YAML-defined workflows but requires Kubernetes, making local testing challenging.
- **Kubeflow and Metaflow**: Abstract away infrastructure complexities, enabling seamless dev and prod workflows. Kubeflow relies on Argo and Kubernetes, while Metaflow simplifies the process with Python decorators.

**Key Takeaways**
- **Schedulers and Orchestrators**: Essential for managing repetitive and dependent ML tasks, optimizing resource utilization, and ensuring smooth execution.
- **Workflow Management Tools**: Provide advanced capabilities for defining, scheduling, and orchestrating ML workflows, with varying degrees of complexity and ease of use.
- **Cloud Dev Environments**: Offer significant benefits in standardization, IT support, remote work capabilities, and bridging the gap between development and production environments. 

By leveraging these tools and principles, organizations can effectively manage resources, automate workflows, and improve the productivity of their ML engineering teams.

##### 10.3 ML Platform

**Introduction**:
   - An ML platform is essential for deploying ML applications efficiently.
   - Initial focus often on specific applications like recommender systems, but these tools can be generalized for broader ML use.

**Components**:
   - **Model Development**: Tools for creating and training models.
   - **Model Store**: Centralized storage for model artifacts.
   - **Feature Store**: Manages and serves features used by models.

**Key Considerations**:
   - **Cloud Provider Compatibility**: Ensures integration with existing cloud infrastructure.
   - **Open Source vs. Managed Services**: Balance between control and maintenance overhead versus convenience and compliance.

*Model Deployment*

1. **Purpose**:
   - Making trained models accessible to users through endpoints.
   - Supports both online and batch predictions.

2. **Tools**:
   - Major cloud providers like AWS, GCP, Azure offer deployment tools.
   - Startups also provide specialized deployment tools like MLflow, Seldon, and Ray Serve.

3. **Challenges**:
   - Ensuring model quality before deployment using techniques like shadow deployment, canary release, and A/B testing.

*Model Store*

1. **Importance**:
   - Stores models and associated artifacts for easier debugging and maintenance.

2. **Artifacts to Store**:
   - Model definition, parameters, featurize and predict functions, dependencies, data, model generation code, experiment artifacts, tags.

3. **Challenges**:
   - Need for comprehensive storage solutions that include various artifacts for effective model management.
   - Existing solutions like MLflow are popular but have limitations in handling artifacts efficiently.

*Feature Store*

1. **Purpose**:
   - Addresses feature management, computation, and consistency across ML models.

2. **Key Functions**:
   - **Feature Management**: Sharing and discovering features across models.
   - **Feature Computation**: Efficiently computing and storing features.
   - **Feature Consistency**: Ensuring features are consistent between training and inference.

3. **Current Landscape**:
   - Feature stores are a newer category with varying capacities among different vendors.
   - Popular solutions include Feast (focus on batch features) and Tecton (handles both batch and online features).

By implementing a comprehensive ML platform with robust model and feature stores, organizations can streamline their ML workflows, ensure consistency, and maintain high-quality deployments.

##### 10.4 Build Versus Buy

*Infrastructure for ML Needs*

1. **Infrastructure Requirements**:
   - Vary based on application type and scale.
   - Investment depends on whether infrastructure is built in-house or outsourced.

2. **Outsourcing vs. In-House**:
   - **Outsourcing**: Minimal infrastructure needed, mainly for data movement.
   - **In-House**: Requires extensive infrastructure and possibly even data centers.

3. **Hybrid Approach**:
   - Most companies use a mix of in-house and outsourced components.
   - Common setup: managed compute by AWS EC2, data warehouse by Snowflake, in-house feature store, and monitoring dashboards.

*Factors in Build vs. Buy Decisions*

1. **Company Stage**:
   - Early stages: Use vendor solutions for quick setup.
   - Growth stages: Shift to in-house solutions as vendor costs become prohibitive.

2. **Focus and Competitive Advantage**:
   - Companies prioritize building in-house for core competencies.
   - Non-tech companies often prefer managed services.
   - Tech companies may prefer modular, customizable services for better control.

3. **Maturity of Available Tools**:
   - Early adopters build their own infrastructure due to lack of mature solutions.
   - Mature solutions face challenges integrating with custom infrastructures of large tech companies.
   - Advisement against selling to big tech due to "integration hell," focusing on startups instead.

*Cost Considerations*

1. **Misconceptions About Costs**:
   - Building is not always cheaper than buying.
   - Building requires hiring more engineers and may limit future innovation due to integration issues.

2. **Complexity of Decisions**:
   - Build vs. buy decisions are complex and context-dependent.
   - Importance of vendor/product selection is growing rapidly in the fast-evolving infrastructure space.

#### 11. Infrastructure and tooling for MLops

##### 11.1 User experience


*User Experience with ML Systems*

1. **Differences from Traditional Software**:
   - ML systems are probabilistic, meaning the same input can yield different results at different times.
   - ML predictions are mostly correct but not always, making it hard to predict when they will be accurate.
   - ML systems can be large and slow, impacting prediction times.

2. **Challenges and Solutions**:
   - **Consistency in User Experience**:
     - Users expect consistent behavior from applications.
     - Inconsistency in ML predictions can confuse users.
     - Example: Booking.com used rules to ensure filter recommendations remain consistent under certain conditions to balance accuracy and consistency.
   - **Combatting “Mostly Correct” Predictions**:
     - Large language models like GPT-3 provide mostly correct but sometimes inaccurate predictions.
     - For some tasks, showing multiple predictions can help users find correct ones.
     - This approach, called "human-in-the-loop" AI, involves humans refining or selecting from AI-generated predictions.
   - **Smooth Failing**:
     - ML models can have variable inference times, especially with complex queries.
     - Companies use backup systems with simpler models or precomputed predictions for quick responses when main models are slow.
     - This addresses the speed-accuracy trade-off, ensuring timely responses even if accuracy is slightly compromised.

*Detailed Points*

1. **Ensuring User Experience Consistency**:
   - Users are accustomed to consistent interfaces and functionalities.
   - ML predictions’ variability can disrupt this consistency.
   - Solutions involve setting rules for when to provide consistent versus new predictions, as illustrated by Booking.com's filter recommendation system.

2. **Combatting “Mostly Correct” Predictions**:
   - Large language models are versatile but not always accurate.
   - Fine-tuning for specific tasks is costly.
   - Showing multiple predictions and allowing users to choose or refine them increases utility, especially for non-expert users.

3. **Smooth Failing**:
   - Handling variable inference times involves backup systems.
   - These systems use simpler models or cached results for quick responses.
   - Backup models can be triggered by rules or additional models predicting main model latency, addressing the need for fast responses in user-critical scenarios.

##### 11.2 Team structure

**Cross-functional Teams Collaboration**:
   - ML projects involve diverse stakeholders: data scientists, ML engineers, DevOps engineers, platform engineers, and subject matter experts (SMEs).
   - **Role of SMEs**:
     - Integral for the entire ML lifecycle, not just data labeling.
     - Involved in problem formulation, feature engineering, error analysis, model evaluation, reranking predictions, and user interface design.
   - **Challenges**:
     - Communicating ML limitations to non-engineering SMEs.
     - Versioning domain expertise into code.
     - Involving SMEs early and using no-code/low-code platforms for SME contributions.

**End-to-End Data Scientists**:
   - ML production requires both ML and operational expertise (deployment, containerization, job orchestration, workflow management).
   - Companies use two main approaches to integrate these skills:

**Approach 1: Separate Teams for Development and Production**:
   - Data science team develops models; Ops/platform team productionizes them.
   - **Advantages**:
     - Easier hiring due to specialized skill sets.
     - Simplifies focus for individuals.
   - **Drawbacks**:
     - Communication and coordination overhead.
     - Debugging challenges and potential for finger-pointing.
     - Limited visibility into the entire process, reducing optimization opportunities.

**Approach 2: Data Scientists Own the Entire Process**:
   - Data scientists manage both development and production.
   - **Advantages**:
     - End-to-end ownership can streamline processes.
   - **Drawbacks**:
     - Requires a wide range of skills, making it challenging for data scientists.
     - Risk of spending more time on infrastructure tasks than data science.

**Importance of Tools**:
   - Effective tools can help data scientists manage the end-to-end process without deep infrastructure knowledge.
   - **Example**: Tools that manage infrastructure details (e.g., data storage, dependencies, code execution) can help data scientists focus on ML tasks.

**Full-Stack Data Scientists**:
   - The success of full-stack data scientists depends on tools that abstract complex infrastructure tasks.
   - **Netflix's Model**: Specialists create tools that automate parts of the process, enabling data scientists to manage projects end-to-end.

**Key Considerations**:
   - Good tools and infrastructure are crucial for efficient ML projects.
   - Organizational structure impacts productivity and effectiveness in ML projects.
   - The balance between specialized teams and end-to-end ownership is context-dependent.

