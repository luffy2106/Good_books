### The summary of the book Machine Learning System Design - Huyen Chip

#### 3. Data engineering fundamentals
##### 3.1 Data sources

- ML systems use data from multiple sources with varying characteristics and purposes, requiring different processing methods.
- Understanding data sources enhances efficient data usage in ML systems.
- Key data sources include:
  - **User Input Data:** 
    - Includes text, images, videos, and uploaded files.
    - Prone to malformation and requires thorough validation and fast processing.
  - **System-Generated Data:**
    - Includes logs and system outputs like model predictions.
    - Provides visibility for debugging and improvement.
    - Typically less malformatted and can be processed periodically.
    - Can grow quickly, creating challenges in identifying relevant information and storage.
    - Logging extensively is common for thorough debugging, with services like Logstash, Datadog, and Logz.io aiding in log analysis.
  - **Behavioral Data:**
    - Records user actions (clicks, scrolling, etc.) and is subject to privacy regulations.
  - **Internal Databases:**
    - Generated by services and applications within a company.
    - Used by ML models for tasks like inventory checks and search query processing.
  - **Third-Party Data:**
    - Includes data collected by other companies or public data, often requiring payment.
    - Used for insights like social media activities, purchase history, and demographic analysis.
    - Privacy regulations (like Apple’s IDFA opt-in policy) have reduced availability, pushing companies to rely more on first-party data.
    - Advertisers are developing workarounds, such as the China Advertising Association’s CAID system for device fingerprinting.

##### 3.2 Data formats

- **Data Storage Considerations:**
  - Storing data from multiple sources can be complex and costly.
  - Consider the future use of data to choose an appropriate storage format.
  - Key questions include:
    - How to store multimodal data (images, text).
    - Cost-effective yet fast data storage options.
    - Storing complex models for different hardware compatibility.

- **Data Serialization:**
  - Converts data structures into a storable/transmittable format.
  - Important characteristics of serialization formats: human readability, access patterns, text vs. binary.

- **Common Data Formats:**
  - **JSON:** Text-based, human-readable, widely used, but schema changes are difficult.
  - **CSV:** Text-based, row-major format, better for accessing rows.
  - **Parquet:** Binary, column-major format, better for accessing columns.

- **Row-Major vs. Column-Major Formats:**
  - Row-major (e.g., CSV) is faster for row access and data writes.
  - Column-major (e.g., Parquet) is faster for column-based reads and large datasets.

- **Text vs. Binary Formats:**
  - Text files (e.g., CSV, JSON) are human-readable but larger.
  - Binary files (e.g., Parquet) are more compact and efficient.

- **Relational Data Model:**
  - Organizes data into tables (relations) with rows (tuples).
  - Normalization reduces redundancy but spreads data across tables.
  - SQL is the primary query language, declarative in nature, with query optimization being crucial.

- **Declarative ML Systems:**
  - Inspired by declarative data systems, aim to simplify ML tasks.
  - Examples: Ludwig (Uber), H2O AutoML, which automate model selection and training.

- **NoSQL Data Models:**
  - **Document Model:**
    - Stores data in self-contained documents (e.g., JSON, BSON).
    - Flexible, schemaless but shifts schema assumption to the reading application.
  - **Graph Model:**
    - Focuses on relationships between data items, using nodes and edges.
    - Efficient for relationship-based queries.

- **Structured vs. Unstructured Data:**
  - **Structured Data:**
    - Follows a predefined schema, easier to analyze.
    - Stored in data warehouses(ex : Google BigQuerry, SnowFlake)
  - **Unstructured Data:**
    - Does not follow a schema, more flexible.
    - Stored in data lakes(ex: Amazon S3 (Simple Storage Service) with AWS Lake Formation, Azure Data Lake Storage (ADLS))
  - Data warehouses are for processed data; data lakes store raw data.

##### 3.3 Data Storage Engines and Processing

- **Data Storage Engines and Processing:**
  - Data formats and models define how data is stored/retrieved.
  - Storage engines (databases) implement these processes.
  - Understanding different databases is crucial for application-specific needs.

- **Transactional vs. Analytical Processing:**
  - **Transactional Processing (OLTP):**
    - Handles individual actions (e.g., tweeting, ordering a ride).
    - Requires low latency and high availability.
    - Often uses ACID principles (Atomicity, Consistency, Isolation, Durability).
  - **Analytical Processing (OLAP):**
    - Handles data aggregation and analysis.
    - Efficient for queries involving data from multiple rows/columns.
  - Modern databases often blend OLTP and OLAP capabilities (e.g., CockroachDB, Apache Iceberg, DuckDB).

- **ETL (Extract, Transform, Load) Process:**
  - Essential for ML system data handling.
  - **Extract:**
    - Retrieve data from various sources, validate, and reject corrupted data.
  - **Transform:**
    - Clean, standardize, and process data (joining, deduplicating, aggregating).
  - **Load:**
    - Load transformed data into target destinations (databases, data warehouses).
  - ETL is foundational for data processing in organizations.

- **Evolution of Data Storage:**
  - Growth of data sources and changing schemas led to storing raw data in data lakes.
  - ELT (Extract, Load, Transform) stores data first, processes later, allowing fast data arrival.
  - Challenges with raw data search inefficiency and evolving infrastructures have led to hybrid solutions combining data lakes and warehouses (e.g., Databricks, Snowflake's data lakehouse).

- **Key Concepts:**
  - **Transactional Databases:**
    - Typically row-major, efficient for individual transactions but not for aggregating data.
  - **Analytical Databases:**
    - Efficient for aggregating and analyzing large datasets.
  - **Decoupling Storage from Processing:**
    - Allows storing data in one place with different processing layers for various queries.

- **Terminology:**
  - **ACID vs. BASE:**
    - ACID: Strong consistency and reliability.
    - BASE: More flexibility with eventual consistency.
  - **Online, Nearline, Offline Processing:**
    - Online: Immediate data availability.
    - Nearline: Quick data availability without human intervention.
    - Offline: Requires human intervention for data access.

##### 3.4 Dataflow Modes

- **Dataflow Modes:**
  - **Data Passing Through Databases:**
    - Data is written by one process and read by another from a shared database.
    - Limitations: both processes must access the same database and database read/write speeds may not meet latency requirements.
  
  - **Data Passing Through Services:**
    - Data is exchanged through network requests (e.g., REST, RPC).
    - Used in service-oriented and microservice architectures.
    - Allows independent development and maintenance of different application components.
    - Examples: Ride-sharing app services (driver management, ride management, price optimization).
  
  - **Data Passing Through Real-Time Transport:**
    - Data is broadcast to an intermediary broker rather than directly between services.
    - Reduces complexity and dependency issues in interservice communication.
    - More efficient for data-heavy systems with strict latency requirements.
    - Examples of real-time transport: Apache Kafka, Amazon Kinesis (pubsub), Apache RocketMQ, RabbitMQ (message queue).

- **Request-Driven Architecture:**
Request-driven architecture, often seen in RESTful APIs and microservices architectures, focuses on systems where clients send requests to services, and services respond with the requested data or action. This type of architecture is synchronous, meaning the client waits for the server to process the request and return a response. Here's an example to illustrate request-driven architecture:
    - User Action: A user opens the ShopSmart app and searches for "smartphones."
    - Client Request: The frontend app sends a GET request to the API Gateway endpoint /products?search=smartphones.
    - API Gateway: Forwards the request to the Product Service.
    - Product Service: Queries the database for products matching "smartphones" and returns the list to the API Gateway.
    - Response: The API Gateway sends the product list back to the frontend app, displaying it to the user.
Key features:
    - Services communicate via direct requests.
    - Suitable for logic-heavy systems.
    - Potential issues: complexity in communication, dependency on service availability, and potential bottlenecks.
- **Event-Driven Architecture:**
  - Uses brokers to manage data passing (in-memory storage for real-time transport).
  - Suitable for data-heavy systems.
  - Data broadcast to a real-time transport is called an event; the system is also called an event bus.
  - Pubsub model: Services publish to topics and any subscribing service can read the events.
  - Message queue model: Events (messages) have intended consumers, and the queue ensures delivery to the right consumers.
  
- **Examples and Popular Implementations:**
  - **Pubsub Solutions:**
  In a pub-sub model, message senders (publishers) send messages to a topic, and multiple receivers (subscribers) receive those messages by this topic. This model decouples the producers and consumers of messages, allowing for a more flexible and scalable architecture.
  Use cases example:
  * Publisher: User actions such as posting a status update or liking a photo.
  * Subscribers: Followers of the user who need to be notified of the update
  * Flow:
    - When a user posts a status update, the action is published to a topic.
    - All followers of the user are subscribed to this topic and receive notifications in real-time.  
  Tool example:
    - Apache Kafka
    - Amazon Kinesis
  - **Message Queue Solutions:**
  In a message queue model, messages are stored in a queue and processed by consumers in a first-in, first-out (FIFO) order. This model is designed for point-to-point communication, ensuring that each message is processed by exactly one consumer.
  Use Case Example: Order Processing System in an E-Commerce App:
  * Producer: The order placement service that generates new orders.
  * Consumer: The order fulfillment service that processes and ships orders.
  * Flow:
    - When a customer places an order, the order details are sent to a message queue.
    - The order fulfillment service reads the order from the queue and processes it.
    - The message is removed from the queue once it is successfully processed.
  Another example could be taken is E-cataloging process, when we ask the users validate the result of the model
  Tool example:
    - Apache RocketMQ
    - RabbitMQ
  - **The main difference PubSub and Mesage Queue**
    - Pub-Sub Solutions: Ideal for scenarios where messages need to be broadcast to multiple subscribers in real-time, such as notifications or real-time updates.
    - Message Queue Solutions: Best suited for scenarios requiring reliable, point-to-point communication with message persistence and load balancing, such as task processing or order fulfillment.

- **Advantages of Real-Time Transport:**
  - Simplifies communication between services.
  - Decouples service dependencies.
  - Enhances scalability and resilience.
  - Ensures timely data availability for applications with strict latency requirements.

##### 3.5 Batch Processing Versus Stream Processing 
- **Historical Data vs. Streaming Data:**
  - **Historical Data:** Stored in data storage engines (databases, data lakes, data warehouses) and processed in batch jobs.
  - **Streaming Data:** Continuously flows through real-time transports like Apache Kafka and Amazon Kinesis, processed in real-time.

- **Batch Processing:**
  - Processes historical data periodically (e.g., daily).
  - Efficient for static features (e.g., driver ratings).
  - Uses distributed systems like MapReduce and Spark.
  - Suitable for features that change less frequently.
  - Known as static features.

- **Stream Processing:**
  - Processes streaming data in real-time or in short intervals (e.g., every five minutes).
  - Ideal for dynamic features (e.g., current number of available drivers).
  - Low latency as it processes data as soon as it is generated.
  - Uses scalable technologies like Apache Flink, KSQL, and Spark Streaming.
  - Suitable for features that change quickly.
  - Known as dynamic features.

- **Advantages of Stream Processing:**
  - Provides low latency by processing data immediately.
  - Efficient for stateful computation, preventing redundancy.
  - Highly scalable and capable of parallel computation.
  - Suitable for applications requiring frequent updates, such as fraud detection and credit scoring.

- **Challenges in Stream Processing:**
  - Handling unbounded data with variable rates and speeds.
  - More complex than batch processing due to continuous data flow.

- **Combining Batch and Stream Processing:**
  - Many problems require both static (batch) and dynamic (streaming) features.
  - Infrastructure needed to process and join both types of data for ML models.
  - Apache Flink and KSQL provide SQL abstraction, useful for complex stream feature extraction.

- **Stream Computation Engines:**
  - **Apache Kafka:** Built-in stream computation capabilities, limited with various data sources.
  - **Apache Flink and KSQL:** Recognized for efficient stream processing with SQL abstraction.
  - **Spark Streaming:** Another option for stream processing.

- **Conclusion:**
  - Batch processing is simpler but less frequent and suited for static data.
  - Stream processing is complex, more frequent, and suited for dynamic data.
  - Combining both processing types enhances the capability to generate accurate ML model predictions.

#### 4. Training data

##### 4.1 Sampling

- **Importance of Sampling in ML:**
  - Integral to various stages of ML projects: creating training data, data splitting, and monitoring.
  - Necessary when all real-world data isn’t available or when processing all data is infeasible.
  - Sampling methods improve efficiency and help avoid biases.

- **Types of Sampling:**
  - **Nonprobability Sampling:**
    - Convenience sampling: Based on data availability.
    - Snowball sampling: Future samples selected based on existing samples.
    - Judgment sampling: Experts select samples.
    - Quota sampling: Samples based on quotas without randomization.
    - Often biased and not representative of real-world data.
    - Common in initial data gathering for projects.

  - **Probability-Based Sampling:**
    - **Simple Random Sampling:**
      - Equal probability for all samples.
      - Easy to implement but may miss rare categories.
    - **Stratified Sampling:**
      - Divides population into groups (strata) and samples from each.
      - Ensures representation of all groups but can be challenging for multilabel tasks.
    - **Weighted Sampling:**
      - Samples assigned weights to influence selection probability.
      - Useful for domain expertise and adjusting for distribution differences.
      - Related to sample weights in ML for influencing model training.
    - **Reservoir Sampling:**
      - Useful for streaming data.
      - Ensures equal probability for each sample even when the total data size is unknown.
      - Algorithm maintains a reservoir of samples with correct probabilities.
    - **Importance Sampling:**
      - Samples from an easier distribution (Q) and weights them to match the target distribution (P).
      - Common in policy-based reinforcement learning to update policies efficiently.

- **Nonprobability Sampling Examples:**
  - Language modeling often uses easily collected data (e.g., Wikipedia, Reddit).
  - Sentiment analysis uses naturally labeled data (e.g., IMDB, Amazon reviews).
  - Self-driving cars initially collected data from limited geographic areas.

- **Simple Random Sampling Example:**
  - Randomly selecting 10% of a population ensures equal chances but may miss rare categories.

- **Stratified Sampling Example:**
  - Sampling 1% from two classes ensures both are represented, useful in unbalanced datasets.

- **Weighted Sampling Example:**
  - Assigning higher weights to more recent data or underrepresented categories to reflect real-world distribution.

- **Reservoir Sampling Example:**
  - Sampling tweets from an incoming stream without knowing the total number of tweets.

- **Importance Sampling Example:**
  - In reinforcement learning, using old policy rewards as a proposal distribution for new policy updates.

Understanding and choosing the appropriate sampling method is crucial for creating reliable and efficient ML models.


##### 4.2 Labelling

